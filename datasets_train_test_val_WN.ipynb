{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41bf9a50-4351-454f-8b96-c82bd34a10d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save training, testing and validation datasets to be read by jobs on cluster\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "os.chdir(\"/Users/niloughazavi/Documents/GitHub/RetinaPredictors-main\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "from model.data_handler import check_trainVal_contamination\n",
    "from model.data_handler_mike import load_data_allLightLevels_cb, load_data_allLightLevels_natstim, save_h5Dataset\n",
    "from collections import namedtuple\n",
    "Exptdata = namedtuple('Exptdata', ['X', 'y'])\n",
    "Exptdata_spikes = namedtuple('Exptdata', ['X', 'y','spikes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac6c4a-04a3-437a-9a00-c826b94ebc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lightLevel = 'allLightLevels'     # ['scotopic', 'photopic','scotopic_photopic']\n",
    "datasetsToLoad = ['mesopic',]#,'photopic'];    #['scotopic','photopic','scotopic_photopic']\n",
    "N_split = 0\n",
    "\n",
    "\n",
    "natstim_idx_val =0\n",
    "\n",
    "# Nilou changed it to 1 (it was 6)\n",
    "REP_TRAINING_DATA = 0\n",
    "STIM = 'NATSTIM'+str(natstim_idx_val)+'_CORR' # 'CB'  'NATSTIM'\n",
    "STIM_NAT = 'NATSTIM'+str(natstim_idx_val)+'_CORR'\n",
    "file_suffix = 'Rstar'\n",
    "NORM_STIM = 0\n",
    "NORM_RESP = True\n",
    "D_TYPE = 'f4'\n",
    "\n",
    "\n",
    "expDate = '20230725C'     \n",
    "path_dataset = os.path.join('D:\\\\Nilou\\\\GitHub\\\\Gradients\\\\RGC_Selective_Simulation\\\\Natural_Movies\\\\analyses_parasol_midget_84cells\\\\data_mike_nat',expDate,'datasets')\n",
    "path_save = os.path.join('D:\\\\Nilou\\\\GitHub\\\\Gradients\\\\RGC_Selective_Simulation\\\\Natural_Movies\\\\analyses_parasol_midget_84cells\\\\data_mike_nat',expDate,'datasets')\n",
    "# path_dataset = os.path.join('/home/saad/postdoc_db/analyses/data_kiersten/',expDate,'gradient_analysis/datasets/')\n",
    "    \n",
    "t_frame = 8\n",
    "\n",
    "\n",
    "fname_dataFile = os.path.join(path_dataset,(expDate+'_dataset_'+STIM+'_allLightLevels'+'_'+str(t_frame)+'ms_'+file_suffix+'.h5'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filt_temporal_width = 0\n",
    "idx_cells = None\n",
    "thresh_rr = 0\n",
    "\n",
    "\n",
    "frac_val = 0.05\n",
    "frac_test = 0.01 \n",
    "\n",
    "\n",
    "    \n",
    "def stim_vecToMat(data,num_checkers_y,num_checkers_x):\n",
    "    X = data.X\n",
    "    X = np.reshape(X,(X.shape[0],num_checkers_y,num_checkers_x),order='F')    # convert stim frames back into spatial dimensions\n",
    "    data = Exptdata_spikes(X,data.y,data.spikes)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# % Calculate grand response median\n",
    "totalNum_units =84\n",
    "spikerate_grand = np.empty((totalNum_units,0))\n",
    "\n",
    "STIMS_ALL = ('NATSTIM'+str(natstim_idx_val)+'_CORR' ,'CB_CORR')\n",
    "\n",
    "\n",
    "#STIMS_ALL = ('NATSTIM'+str(natstim_idx_val))\n",
    "\n",
    "\n",
    "# for i in range(1):\n",
    "#     STIM = STIMS_ALL[i]  # Set STIM to the current element in STIMS_ALL\n",
    "#     fname = os.path.join(path_dataset, (expDate + '_dataset_' + STIM + '_allLightLevels' + '_' + str(t_frame) + 'ms_' + file_suffix + '.h5'))\n",
    "#     print(f\"Looking for file: {fname}\")\n",
    "    \n",
    "#     if not os.path.exists(fname):\n",
    "#         print(f\"File not found: {fname}\")\n",
    "#         continue\n",
    "    \n",
    "#     with h5py.File(fname,'r') as f:\n",
    "#         spikerate_grand = np.concatenate((spikerate_grand,np.array(f['spikerate_grand'])),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "#STIM = STIMS_ALL # Set STIM to the current element in STIMS_ALL\n",
    "fname = os.path.join(path_dataset, (expDate + '_dataset_' + STIM + '_allLightLevels' + '_' + str(t_frame) + 'ms_' + file_suffix + '.h5'))\n",
    "print(f\"Looking for file: {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(fname,'r') as f:\n",
    "    spikerate_grand = np.concatenate((spikerate_grand,np.array(f['spikerate_grand'])),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "spikerate_grand[spikerate_grand==0]=np.nan\n",
    "resp_med_grand = np.nanmedian(spikerate_grand,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fc37e-3eaf-417a-90ed-2e0ab1e140e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasetsToLoad[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasetsToLoad:\n",
    "    fname_noise = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM_NAT+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'+'.h5'))\n",
    "\n",
    "\n",
    "    \n",
    "    if STIM[:7] == 'NATSTIM':\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig,_ = load_data_allLightLevels_natstim(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM=False,NORM_RESP=NORM_RESP)\n",
    "\n",
    "\n",
    "        if REP_TRAINING_DATA>0:\n",
    "            print('RESAMPLING TRAINING SAMPLES')\n",
    "            X = np.tile(data_train.X,[REP_TRAINING_DATA,1,1,1,1])\n",
    "            y = np.tile(data_train.y,[REP_TRAINING_DATA,1,1,1])\n",
    "            spikes = np.tile(data_train.spikes,[REP_TRAINING_DATA,1,1,1])\n",
    "            \n",
    "            data_train = Exptdata_spikes(X,y,spikes)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        # if data_quality['var_noise']==None:\n",
    "        #     with h5py.File(fname_noise) as f:\n",
    "        #         obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        #     data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "            \n",
    "    elif 'CB' in STIM:\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig = load_data_allLightLevels_cb(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM = False,NORM_RESP=NORM_RESP)\n",
    "        \n",
    "        \n",
    "        with h5py.File(fname_noise) as f:\n",
    "            obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "    if REP_TRAINING_DATA>0:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_REP-'+str(REP_TRAINING_DATA)+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    else:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    \n",
    "    f = h5py.File(fname_dataFile,'r')\n",
    "    samps_shift = 0#np.array(f[dataset]['val']['spikeRate'].attrs['samps_shift'])\n",
    "    if 'num_checkers_x' in f[dataset]['train']['stim_frames'].attrs.keys():\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_x'])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_y'])\n",
    "        checkSize_um = np.array(f[dataset]['train']['stim_frames'].attrs['checkSize_um'])\n",
    "    else:\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].shape[2])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].shape[1])\n",
    "        checkSize_um = 3.8  # 3.8 um/pixel\n",
    "\n",
    "\n",
    "    t_frame_inData = np.array(f[dataset]['train']['stim_frames'].attrs['t_frame'])\n",
    "    parameters = {\n",
    "    't_frame': t_frame_inData,\n",
    "    'filt_temporal_width': filt_temporal_width,\n",
    "    'frac_val': frac_val,\n",
    "    'frac_test':frac_test,\n",
    "    'thresh_rr': thresh_rr,\n",
    "    'samps_shift': samps_shift,\n",
    "    'num_checkers_x': num_checkers_x,\n",
    "    'num_checkers_y': num_checkers_y,\n",
    "    'checkSize_um': checkSize_um\n",
    "    }\n",
    "    f.close()\n",
    "    \n",
    "    if data_train.X.ndim == 2:\n",
    "       data_train = stim_vecToMat(data_train,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_val = stim_vecToMat(data_val,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_test = stim_vecToMat(data_test,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "    \n",
    "    # fname_data_train_val_test = fname_data_train_val_test + '_StimNorm-'+str(NORM_STIM) #+ '_RespNorm-'+str(NORM_RESP)\n",
    "\n",
    "\n",
    "    save_h5Dataset(fname_data_train_val_test+'.h5',data_train,data_val,data_test,data_quality,dataset_rr,parameters,resp_orig=resp_orig,dtype=D_TYPE)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
