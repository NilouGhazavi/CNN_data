{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e2e17a-e045-48ff-9f34-f9ac3796d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy import io\n",
    "import re\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# from global_scripts import spiketools\n",
    "import gc\n",
    "# import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import math\n",
    "import hdf5storage\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/niloughazavi/Documents/GitHub/CNN_data')\n",
    "from WN_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcd91eff-a284-48c8-ac82-2d075a5e2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expDate = '20240229C'\n",
    "\n",
    "UP_SAMP = 0\n",
    "bin_width = 8    # ms\n",
    "sig = 4      #2 for 16 ms; 4 for 8 ms; 8 for 4 ms; 20 for 1 ms\n",
    "\n",
    "CONVERT_RSTAR = True\n",
    "NORM_STIM = 1\n",
    "NORM_RESP = True\n",
    "FRAME_RESIZE = 8\n",
    "stim_upsample = 2 #2\n",
    "t_frame = (1/60.3180)*1000/stim_upsample    # because spikerate is sampled at 120 but stim at 60Hz. So we will upsample stim later. So each frame is 8 ms\n",
    "\n",
    "# Load the data.\n",
    "lightLevel_text = ['scotopic','mesopic','photopic']\n",
    "lightLevel_mul = [0.5,50,6000]\n",
    "\n",
    "# index of validation movie\n",
    "total_num_movies=9\n",
    "idx_movs_val = [0]      \n",
    "idx_movs_train = np.setdiff1d(np.arange(0,total_num_movies),idx_movs_val)\n",
    "\n",
    "\n",
    "path_raw = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'raw')\n",
    "path_movs = os.path.join(path_raw,'updated_movie_files')\n",
    "path_oldmovs=os.path.join(path_raw,'bugged_movie_files')\n",
    "path_db = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "path_save = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "\n",
    "\n",
    "\n",
    "if CONVERT_RSTAR==True:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_NATSTIM'+str(idx_movs_val[0])+'_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms'+'_Rstar.h5'))\n",
    "else:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_NATSTIM'+str(idx_movs_val[0])+'_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms.h5'))\n",
    "\n",
    "\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f17d7265-59f6-4609-b1bd-fbf94497c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VAL = len(idx_movs_val)   # 2 movies for validation purpose\n",
    "\n",
    "\n",
    "# update this part of the code so that \n",
    "\n",
    "mesopic_cell_dove = {\n",
    "'OffM': [11, 78, 158, 347, 423, 636, 643, 661, 678, 712, 732, 748, 783, 787, 809],\n",
    "'OffP': [5,52,138,360, 498, 49, 632, 648, 713, 721, 726, 741,785],\n",
    "'OnM': [44, 47, 48, 56, 65, 84, 90, 93, 102, 120, 121,136, 142, 150, 183, 197, 225, 233, 250, 261, 284, 294, 318, 384, 440, 460, 494, 497, 511, 515, 520, 534, 538, 549, 572, 595, 619, 769, 792, 819],\n",
    "'OnP': [140, 172, 196, 207, 223, 367, 445, 533, 539, 571, 630, 651, 752, 764, 969, 980],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "mesopic_off_m = mesopic_cell_dove['OffM']\n",
    "mesopic_off_p = mesopic_cell_dove['OffP']\n",
    "mesopic_on_m = mesopic_cell_dove['OnM']\n",
    "mesopic_on_p = mesopic_cell_dove['OnP']\n",
    "\n",
    "\n",
    "# only parasol cells \n",
    "uname_all = list()\n",
    "ctr = 0\n",
    "for i in mesopic_on_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'on_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "    \n",
    "ctr = 0\n",
    "for i in mesopic_off_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'off_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "ctr = 0\n",
    "for i in mesopic_on_m:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'on_m_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "ctr = 0\n",
    "for i in mesopic_off_m:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'off_m_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "\n",
    "\n",
    "totalNum_units = len(mesopic_on_p) + len(mesopic_off_p)+len(mesopic_on_m)+len(mesopic_off_m)\n",
    "\n",
    "\n",
    "mesopic_ids=mesopic_on_p+mesopic_off_p+mesopic_on_m+mesopic_off_m\n",
    "mesopic_ids=np.array(mesopic_ids)\n",
    "\n",
    "idx_allunits=mesopic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13848d-1209-4b43-ac8e-c36d4b8c7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikerate_grand = np.empty((0,totalNum_units,9,10))      # [time,cells,stims,trials]   <-- nee to find a way to do this auto\n",
    "lightLevel_idx = 1\n",
    "\n",
    "for lightLevel_idx in [1]:     #[0=0.3; 1=3; 2=30]\n",
    "    select_lightLevel = lightLevel_text[lightLevel_idx]\n",
    "\n",
    "    fname = os.path.join(path_raw,expDate+'_Doves_'+select_lightLevel+'.pkl')\n",
    "    with open(fname,'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    #---- Stim \n",
    "    \n",
    "    spike_dict = params['spike_dict']\n",
    "    cluster_id = params['cluster_id']\n",
    "    stimulus_index = params['stimulusIndex'].astype(int)\n",
    "    background_intensity = params['backgroundIntensity'].astype(float)\n",
    "    frame_times = params['frame_times'] # This is the time of each frame in milliseconds.\n",
    "    pre_pts = params['pre_pts']\n",
    "    stim_pts = params['stim_pts']\n",
    "    tail_pts = params['tail_pts']\n",
    "    wait_time = params['waitTime']\n",
    "    bin_rate = params['bin_rate'] # This is the bin rate for the spike times in Hz.\n",
    "\n",
    "\n",
    "    unique_movs = np.unique(stimulus_index)\n",
    "    N_TRIALS = int(len(stimulus_index)/len(unique_movs))\n",
    "    frames_mat = np.zeros((362,int(600/FRAME_RESIZE),int(800/FRAME_RESIZE),len(unique_movs)));frames_mat[:] = np.nan        # [time,y/8,x/8,img,trial]\n",
    "    spikes_mat = np.zeros((len(unique_movs),N_TRIALS),dtype='object')\n",
    "    ctr_trial = -1*np.ones(N_TRIALS,dtype='int')\n",
    "    \n",
    "    # Loop over movies\n",
    "    m=5\n",
    "    for m in tqdm(range(len(unique_movs))):\n",
    "        movie_idx = unique_movs[m]\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        \n",
    "        old_movie_path = os.path.join(path_oldmovs, 'doves_idx_' + str(movie_idx) + '.mat')\n",
    "        movie_dic = hdf5storage.loadmat(old_movie_path)\n",
    "\n",
    "        single_movie_path = os.path.join(path_movs, 'doves_idx_' + str(movie_idx) + '.npy')\n",
    "        frames = np.load(single_movie_path)\n",
    "    \n",
    "        bg_mean = movie_dic['image_mean']\n",
    "        # frames = cv2.resize(frames, dsize=(int(frames.shape[1]/FRAME_RESIZE), int(frames.shape[0]/FRAME_RESIZE)), interpolation=cv2.INTER_LINEAR)\n",
    "        frames = frames[:,::FRAME_RESIZE,::FRAME_RESIZE,0]\n",
    "        frames = (frames+frames.max())/(frames.max()-frames.min())\n",
    "        \n",
    "        frames_new = (frames - bg_mean) / bg_mean       # convert to  contrast\n",
    "        \n",
    "        frames_mat[:,:,:,movie_loc] = frames_new\n",
    "\n",
    "        frame_rate = movie_dic['frame_rate'][0]\n",
    "\n",
    "\n",
    "    if CONVERT_RSTAR == True:\n",
    "        meanIntensity = lightLevel_mul[lightLevel_idx]\n",
    "        frames_rstar = applyLightIntensities(meanIntensity,frames_mat,t_frame)\n",
    "        \n",
    "    stim_frames = np.repeat(frames_rstar,stim_upsample,axis=0).astype('float32')\n",
    "    stim_frames = np.repeat(stim_frames[:,:,:,:,None],N_TRIALS,axis=-1)\n",
    "\n",
    "    # Extract spikes\n",
    "    epoch_count = 0\n",
    "    for epoch_count in tqdm(range(len(stimulus_index))):\n",
    "        movie_idx = int(stimulus_index[epoch_count])\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        ctr_trial[movie_loc] += 1\n",
    "\n",
    "        epoch_frame_times = np.array(frame_times[epoch_count])  # Get the frame times for the epoch.\n",
    "        epoch_frame_times -= epoch_frame_times[0]               # Subtract the first frame.\n",
    "        stim_start = pre_pts[epoch_count]                       # The stimulus starts at the end of pre_pts.\n",
    "        the_wait_is_over = stim_start + wait_time[epoch_count]  # The first movie frame is then presented for the duration of the wait time.\n",
    "        stim_end = stim_start + stim_pts[epoch_count]           # The stimulus then plays for the duration of the stim_pts-wait_time.\n",
    "    \n",
    "        # Get the relevant spikes\n",
    "        spikes_epoch = spike_dict[:,epoch_count]\n",
    "        spikes_stim = []\n",
    "        i=0\n",
    "        for i in range(len(spikes_epoch)):\n",
    "             rgb = spikes_epoch[i]\n",
    "             rgb = rgb[rgb>stim_start]\n",
    "             rgb = rgb[rgb<stim_end]\n",
    "             rgb = rgb - stim_start\n",
    "             rgb = rgb[rgb>0]\n",
    "             spikes_stim.append(rgb)\n",
    "        spikes_mat[movie_loc,ctr_trial[movie_loc]] = spikes_stim\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- spikes\n",
    "    assert len(np.unique(stim_pts))==1, 'stim_pts different for different epochs/movies'\n",
    "\n",
    "    stimLength = stim_frames.shape[0]\n",
    "    \n",
    "    flips = np.arange(0,(stimLength+1)*t_frame,t_frame)\n",
    "    \n",
    "    numStims = len(unique_movs)\n",
    "    spikeRate_cells = np.empty((stimLength,totalNum_units,numStims,N_TRIALS))\n",
    "    spikeCounts_cells = np.empty((stimLength,totalNum_units,numStims,N_TRIALS))\n",
    "    spikeTimes_cells = np.empty((totalNum_units,numStims,N_TRIALS),dtype='object')\n",
    "    \n",
    "    \n",
    "    ctr_units = -1\n",
    "    U = 0\n",
    "    # had to be modified for one light level\n",
    "    for U in range(0, len(idx_allunits)):\n",
    "        ctr_units += 1\n",
    "        cluster_unit = idx_allunits[U]\n",
    "        idx_unit_array = np.where(cluster_id == cluster_unit)[0]\n",
    "        # if len(idx_unit_array) == 0:\n",
    "        #     continue  # Skip if cluster_unit is not found in cluster_id\n",
    "        idx_unit = idx_unit_array[0]  # Convert array to integer\n",
    "        \n",
    "        tr = 0\n",
    "        s = 0\n",
    "        for s in range(numStims):\n",
    "            for tr in range(N_TRIALS):\n",
    "                startTime = 0  # Start time for stimulus\n",
    "                endTime = startTime + flips[-1]  # End time for stimulus\n",
    "                \n",
    "                spikes = spikes_mat[s, tr][idx_unit]\n",
    "                if len(spikes) == 0:\n",
    "                    spikes = 0\n",
    "                    spikeCounts = np.zeros(stimLength)\n",
    "                else:\n",
    "                    spikeCounts = np.histogram(spikes, flips)[0]  # Bin the spikes\n",
    "                spikeRate = MEA_spikerates_binned(spikeCounts, sig)  # Calculate spike rates\n",
    "                \n",
    "                spikeRate_cells[:, ctr_units, s, tr] = spikeRate\n",
    "                spikeCounts_cells[:, ctr_units, s, tr] = spikeCounts\n",
    "                spikeTimes_cells[ctr_units, s, tr] = spikes\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalize responses if required\n",
    "    if NORM_RESP:\n",
    "        rgb = np.squeeze(spikeRate_cells)\n",
    "        rgb[rgb == 0] = np.nan\n",
    "        # resp_median = np.nanmedian(rgb, axis=0)\n",
    "        # nilou added\n",
    "        if np.all(np.isnan(rgb)):\n",
    "            resp_median = np.zeros_like(rgb[0])\n",
    "        else:\n",
    "            resp_median = np.nanmedian(rgb, axis=0)\n",
    "        resp_norm = rgb / resp_median[None, :]\n",
    "        resp_norm[np.isnan(resp_norm)] = 0\n",
    "        resp_orig = spikeRate_cells\n",
    "    else:\n",
    "        resp_norm = spikeRate_cells\n",
    "        resp_orig = spikeRate_cells\n",
    "\n",
    "    # Prepare training and validation datasets\n",
    "    idx_frames_start = 0  # Discard first 121 frames\n",
    "    stim_frames_train = (stim_frames[idx_frames_start:, :, :, idx_movs_train, :], flips[idx_frames_start:])\n",
    "    spikeRate_train = (resp_norm[idx_frames_start:, :, idx_movs_train, :], spikeCounts_cells[idx_frames_start:, :, idx_movs_train, :], spikeTimes_cells[:, idx_movs_train, :])\n",
    "    spikeRate_orig_train = spikeRate_cells[idx_frames_start:, :, idx_movs_train, :]\n",
    "    spikeRate_median_train = resp_median[:, idx_movs_train, :]\n",
    "    unique_movs_train = unique_movs[idx_movs_train]\n",
    "\n",
    "    stim_frames_val = (stim_frames[idx_frames_start:, :, :, idx_movs_val, :], flips[idx_frames_start:])\n",
    "    spikeRate_val = (resp_norm[idx_frames_start:, :, idx_movs_val, :], spikeCounts_cells[idx_frames_start:, :, idx_movs_val, :], spikeTimes_cells[:, idx_movs_val, :])\n",
    "    spikeRate_orig_val = spikeRate_cells[idx_frames_start:, :, idx_movs_val, :]\n",
    "    spikeRate_median_val = resp_median[:, idx_movs_val, :]\n",
    "    unique_movs_val = unique_movs[idx_movs_val]\n",
    "    \n",
    "    # Update grand spikerate\n",
    "    spikerate_grand = np.concatenate((spikerate_grand, spikeRate_cells), axis=0)\n",
    "\n",
    "    # Save dataset\n",
    "    \n",
    "        # Create directories if necessary before saving\n",
    "    os.makedirs(os.path.dirname(fname_save), exist_ok=True)\n",
    "    \n",
    "    with h5py.File(fname_save, 'a') as f:\n",
    "        try:\n",
    "            # Save experimental date and unit information\n",
    "            f.create_dataset('expDate', data=np.array(expDate, dtype='bytes'))\n",
    "            f.create_dataset('units', data=np.array(uname_all, dtype='bytes'))\n",
    "        except:\n",
    "            pass  # If datasets already exist, skip creating them\n",
    "    \n",
    "        # Save training data\n",
    "        grp = f.create_group('/' + select_lightLevel + '/train')\n",
    "        create_dataset(grp, stim_frames_train, spikeRate_train, spikeRate_orig_train, spikeRate_median_train, unique_movs_train, t_frame, N_TRIALS)\n",
    "    \n",
    "        # Save validation data\n",
    "        grp = f.create_group('/' + select_lightLevel + '/val')\n",
    "        create_dataset(grp, stim_frames_val, spikeRate_val, spikeRate_orig_val, spikeRate_median_val, unique_movs_val, t_frame, N_TRIALS)\n",
    "    \n",
    "    # Flatten and reshape spike rate data for saving\n",
    "    spikerate_grand_flattened = np.moveaxis(spikerate_grand, 0, -1)\n",
    "    spikerate_grand_flattened = spikerate_grand_flattened.reshape(spikerate_grand_flattened.shape[0], -1)\n",
    "    resp_median_grand = np.nanmedian(spikerate_grand_flattened, axis=-1)\n",
    "    \n",
    "    # Save grand spike rate and median response data\n",
    "    with h5py.File(fname_save, 'a') as f:\n",
    "        if 'spikerate_grand' in f:\n",
    "            del f['spikerate_grand']  # Delete existing dataset if it exists\n",
    "        if 'resp_median_grand' in f:\n",
    "            del f['resp_median_grand']  # Delete existing dataset if it exists\n",
    "        f.create_dataset('spikerate_grand', data=spikerate_grand_flattened, compression='gzip')\n",
    "        f.create_dataset('resp_median_grand', data=resp_median_grand, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c886d53-ac89-4325-a79d-e8ed6104b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
