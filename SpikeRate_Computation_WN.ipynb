{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44e84785-3c89-46d3-bec0-d3509e0cead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/niloughazavi/Documents/GitHub/RetinaPredictors-main\n",
      "WARNING:tensorflow:From /Users/niloughazavi/anaconda3/envs/cnn_env/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    }
   ],
   "source": [
    "#%% import libraries \n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import io\n",
    "import re\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# from global_scripts import spiketools\n",
    "import gc\n",
    "# import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "# Change the current directory\n",
    "os.chdir(\"/Users/niloughazavi/Documents/GitHub/RetinaPredictors-main\")\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "from model.load_savedModel import *\n",
    "from model.utils_si import *\n",
    "import model.data_handler\n",
    "from model.data_handler import load_data, prepare_data_cnn2d, prepare_data_cnn3d, prepare_data_convLSTM, prepare_data_pr_cnn2d,merge_datasets,isintuple\n",
    "from model.data_handler_mike import load_h5Dataset\n",
    "from model.performance import getModelParams, model_evaluate,model_evaluate_new,paramsToName, get_weightsDict, get_weightsOfLayer,estimate_noise\n",
    "from model import metrics\n",
    "from model import featureMaps\n",
    "from model.models import modelFileName\n",
    "from model.train_model import chunker\n",
    "import model.gradient_tools\n",
    "from model.featureMaps import spatRF2DFit, get_strf, decompose\n",
    "from model.train_model import chunker\n",
    "from model.train_model import chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9f646a-3a4d-4211-83b7-869f3e41215d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/niloughazavi/Documents/GitHub/RetinaPredictors-main\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sys.path.append('/Users/niloughazavi/Documents/GitHub/CNN_data')\n",
    "\n",
    "from WN_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20943172-a55f-49e3-8c76-55154bda6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% exp details\n",
    "expDate = '20240229C'\n",
    "UP_SAMP = 0\n",
    "bin_width = 8   # ms\n",
    "sig = 4#8 # 2   #2 for 16 ms; 4 for 8 ms; 8 for 4 ms; 20 for 1 ms\n",
    "\n",
    "\n",
    "CONVERT_RSTAR = True\n",
    "NORM_STIM = 1\n",
    "NORM_RESP = True\n",
    "\n",
    "\n",
    "path_raw = os.path.join('/Users/niloughazavi/Documents/GitHub/Gradients/RGC_Selective_Simulation/White_Noise/analyses/data_mike_noise',expDate,'raw')\n",
    "path_db = os.path.join('/Users/niloughazavi/Documents/GitHub/Gradients/RGC_Selective_Simulation/White_Noise/analyses/data_mike_noise',expDate,'datasets')\n",
    "path_save = os.path.join('/Users/niloughazavi/Documents/GitHub/Gradients/RGC_Selective_Simulation/White_Noise/analyses/data_mike_noise',expDate,'datasets')\n",
    "if CONVERT_RSTAR==True:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_CB_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms'+'_Rstar.h5'))\n",
    "else:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_CB_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms.h5'))\n",
    "\n",
    "\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)\n",
    "\n",
    "select_colChan = 0\n",
    "stride = 2\n",
    "depth = 61 # You'll need to increase this for the mesopic and scotopic noise runs because the kinetics are slower.\n",
    "\n",
    "# Load the data.\n",
    "lightLevel_text = ['scotopic','mesopic','photopic']\n",
    "lightLevel_mul = [0.5,50,6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f78dd-a3b0-4b38-a5a7-dd4efc32cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the text files\n",
    "\n",
    "mesopic_cell_dove = {\n",
    "    'OffP': [5,49,52,130,137, 138, 258, 316, 360, 444, 498, 521, 575, 594, 632, 648, 688, 713, 721, 741, 785, 998],\n",
    "    'OnP': [12, 20,140, 172, 196, 207, 223, 313, 342, 445, 446, 507, 533, 571, 630, 651, 752, 764, 788,969, 980]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mesopic_off_p = mesopic_cell_dove['OffP']\n",
    "mesopic_on_p = mesopic_cell_dove['OnP']\n",
    "\n",
    "\n",
    "# only parasol cells \n",
    "uname_all = list()\n",
    "ctr = 0\n",
    "for i in mesopic_on_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'on_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "    \n",
    "ctr = 0\n",
    "for i in mesopic_off_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'off_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "totalNum_units = len(mesopic_on_p) + len(mesopic_off_p)\n",
    "\n",
    "\n",
    "mesopic_ids=mesopic_on_p+mesopic_off_p\n",
    "mesopic_ids=np.array(mesopic_ids)\n",
    "\n",
    "idx_allunits=mesopic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0164302-5166-42f9-9304-001925999efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spikerate_grand = np.empty((0,totalNum_units,1))\n",
    "idx_allunits= mesopic_ids                           \n",
    "lightLevel_idx = 1\n",
    "\n",
    "\n",
    "for lightLevel_idx in [1]:     #[0=0.3; 1=3; 2=30]\n",
    "    select_lightLevel = lightLevel_text[lightLevel_idx]\n",
    "\n",
    "    fname = os.path.join(path_raw,expDate+'_Noise_'+select_lightLevel+'.pkl')\n",
    "    with open(fname,'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "\n",
    "\n",
    "    ## PSTH ( bin size= 8ms)\n",
    "    psth = params['psth']\n",
    "    cluster_id = params['cluster_id']\n",
    "    pre_pts = params['pre_pts']\n",
    "    stim_pts = params['stim_pts']\n",
    "    frame_rate = params['frame_rate'] # The precise update rate of the monitor computed from the timing frame flips.\n",
    "    num_checkers_x = int(np.unique(params['numXChecks']))\n",
    "    num_checkers_y = int(np.unique(params['numYChecks']))\n",
    "    checkSize_um = 30\n",
    "\n",
    "    t_frame = (1/frame_rate)*1000/2    # because spikerate is sampled at 120 but stim at 60Hz. So we will upsample stim later. So each frame is 8 ms\n",
    "    \n",
    "    binned_spikes = psth[:,:,pre_pts[0]:pre_pts[0]+int(params['numFrames'][0]*stride)]\n",
    "    \n",
    "    num_units_inDataset = psth.shape[0]\n",
    "\n",
    "    # ---- stim\n",
    "    idx = 1\n",
    "    stim_frames = np.zeros((0,75,100))\n",
    "    spikes = np.zeros((num_units_inDataset,0))\n",
    "    for idx in tqdm(range(len(params['numXStixels'])), desc=''):\n",
    "        frames_temp = get_spatial_noise_frames(\n",
    "            int(params['numXStixels'][idx]),\n",
    "            int(params['numYStixels'][idx]),\n",
    "            int(params['numXChecks'][idx]),\n",
    "            int(params['numYChecks'][idx]),\n",
    "            params['chromaticClass'][idx],\n",
    "            int(params['numFrames'][idx]),\n",
    "            int(params['stepsPerStixel'][idx]),\n",
    "            int(params['seed'][idx]),\n",
    "            int(params['frameDwell'][idx]))\n",
    "        \n",
    "        # frames_temp[frames_temp>0] = 1\n",
    "        # frames_temp[frames_temp<0] = 0\n",
    "        if CONVERT_RSTAR == True:\n",
    "            meanIntensity = lightLevel_mul[lightLevel_idx]\n",
    "            frames_temp = applyLightIntensities(meanIntensity,frames_temp,t_frame)\n",
    "\n",
    "\n",
    "        stim_frames = np.concatenate((stim_frames,frames_temp[:,:,:,select_colChan]))\n",
    "        spikes_temp = binned_spikes[:,idx,:]\n",
    "        spikes = np.concatenate((spikes,spikes_temp),axis=-1)\n",
    "    \n",
    "    \n",
    "    stim_frames = np.reshape(stim_frames,(stim_frames.shape[0],stim_frames.shape[1]*stim_frames.shape[2]),order='F')\n",
    "    \n",
    "    stim_frames = np.repeat(stim_frames,stride,axis=0)\n",
    "    assert stim_frames.shape[0] == spikes.shape[-1],'num of frames does not match num of spikes'\n",
    "\n",
    "\n",
    "    # a = np.unique(stim_frames,axis=0)\n",
    "    \n",
    "    \n",
    "    # ---- spike rates\n",
    "    stimLength = stim_frames.shape[0]\n",
    "    \n",
    "    flips = np.arange(0,(stimLength+1)*t_frame,t_frame)\n",
    "    \n",
    "    numTrials = 1\n",
    "    spikeRate_cells = np.empty((stimLength,totalNum_units,numTrials))\n",
    "    spikeCounts_cells = np.empty((stimLength,totalNum_units,numTrials))\n",
    "    spikeTimes_cells = np.empty((totalNum_units,numTrials),dtype='object')\n",
    "    \n",
    "    \n",
    "    ctr_units = -1\n",
    "    print(\"Total units to process:\", len(idx_allunits))\n",
    "    print(\"Cluster IDs available:\", len(np.unique(cluster_id)))\n",
    "    print(\"\\nStarting unit processing...\")\n",
    "    U=0\n",
    "    skipped_units = []\n",
    "    found_units=[]\n",
    "    for U in range(0, len(idx_allunits)):\n",
    "        ctr_units += 1\n",
    "        cluster_unit = idx_allunits[U]\n",
    "        idx_unit_array = np.where(cluster_id==cluster_unit)[0]\n",
    "        # if len(idx_unit_array) == 0:\n",
    "        #     skipped_units.append(cluster_unit)\n",
    "        #     print(f\"Skipping unit {cluster_unit} - not found in cluster_id\")\n",
    "        #     continue  # Skip if cluster_unit is not found in cluster_id\n",
    "        \n",
    "        # found_units.append(cluster_unit)\n",
    "        idx_unit = idx_unit_array[0]\n",
    "        tr=0\n",
    "        for tr in range(numTrials):\n",
    "            startTime = 0#stimulus_start_times[tr]\n",
    "            endTime = startTime + flips[-1]\n",
    "\n",
    "            # using the PSTH \n",
    "            spikeCounts = spikes[idx_unit]\n",
    "            spikeRate = MEA_spikerates_binned(spikeCounts,sig)\n",
    "            # plt.plot(spikeRate)\n",
    "        \n",
    "            spikeRate_cells[:,ctr_units,tr] = spikeRate\n",
    "            spikeCounts_cells[:,ctr_units,tr] = spikeCounts\n",
    "            spikeTimes_cells[ctr_units,tr] = np.where(spikeCounts)[0]\n",
    "            \n",
    "            \n",
    "            # Add visualization here\n",
    "            fig, axes = plot_cell_response(\n",
    "                cell_idx=idx_unit,\n",
    "                spikes=spikes,\n",
    "                psth=psth,\n",
    "                cluster_id=cluster_id,\n",
    "                sig=sig,\n",
    "                t_frame=t_frame,\n",
    "                frame_rate=frame_rate\n",
    "            )\n",
    "            plt.show()\n",
    "        \n",
    "    if NORM_RESP==True:\n",
    "        rgb = np.squeeze(spikeRate_cells)\n",
    "        rgb[rgb==0]=np.nan\n",
    "        resp_median = np.nanmedian(rgb,axis=0)\n",
    "        resp_norm = rgb/resp_median[None,:]\n",
    "        resp_norm[np.isnan(resp_norm)] = 0\n",
    "        resp_orig = spikeRate_cells\n",
    "    else:\n",
    "        resp_norm = spikeRate_cells\n",
    "        resp_orig = spikeRate_cells\n",
    "    \n",
    "    stim_frames_train = (stim_frames,flips)\n",
    "    spikeRate_train = (resp_norm,spikeCounts_cells,spikeTimes_cells)   # dims [stim_files][0=spikeRate,1=spikeCounts,2=spikeTimes]\n",
    "    spikeRate_orig = spikeRate_cells\n",
    "    spikeRate_median = resp_median\n",
    "    \n",
    "    spikerate_grand = np.concatenate((spikerate_grand,spikeRate_cells),axis=0)\n",
    "\n",
    "    # ---- Save dataset\n",
    "    with h5py.File(fname_save,'a') as f:\n",
    "        try:\n",
    "            f.create_dataset('expDate',data=np.array(expDate,dtype='bytes'))\n",
    "            f.create_dataset('units',data=np.array(uname_all,dtype='bytes'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        grp = f.create_group('/'+select_lightLevel+'/train')\n",
    "        d = grp.create_dataset('stim_frames',data=stim_frames_train[0],compression='gzip')\n",
    "        d.attrs['num_checkers_x'] = num_checkers_x\n",
    "        d.attrs['num_checkers_y'] = num_checkers_y\n",
    "        d.attrs['checkSize_um'] = checkSize_um\n",
    "        d.attrs['t_frame'] = t_frame\n",
    "        \n",
    "        \n",
    "        d = grp.create_dataset('flips_timestamp',data=stim_frames_train[1],compression='gzip')\n",
    "        d.attrs['time_unit'] = 'ms' \n",
    "        \n",
    "        d = grp.create_dataset('spikeRate',data=spikeRate_train[0],compression='gzip')\n",
    "        d.attrs['bins'] = 'bin edges defined by dataset flips_timestamp'\n",
    "        d.attrs['num_units'] = len(uname_all)\n",
    "        d.attrs['sig'] = sig\n",
    "        \n",
    "        d = grp.create_dataset('spikeCounts',data=spikeRate_train[1],compression='gzip')\n",
    "        d.attrs['bins'] = 'bin edges defined by dataset flip_times'\n",
    "        d.attrs['num_units'] = len(uname_all)\n",
    "        \n",
    "        d = grp.create_dataset('spikeRate_orig',data=spikeRate_orig,compression='gzip')\n",
    "        d.attrs['bins'] = 'bin edges defined by dataset flip_times'\n",
    "        d.attrs['num_units'] = len(uname_all)\n",
    "\n",
    "        d = grp.create_dataset('spikeRate_median',data=spikeRate_median,compression='gzip')\n",
    "        d.attrs['bins'] = 'bin edges defined by dataset flip_times'\n",
    "        d.attrs['num_units'] = len(uname_all)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "spikerate_grand_flattened = np.moveaxis(spikerate_grand,0,-1)\n",
    "spikerate_grand_flattened = spikerate_grand_flattened.reshape(spikerate_grand_flattened.shape[0],-1)\n",
    "resp_median_grand = np.nanmedian(spikerate_grand_flattened,axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(fname_save, 'a') as f:\n",
    "    if 'spikerate_grand' in f:\n",
    "        del f['spikerate_grand']  # Delete existing dataset if it exists\n",
    "    if 'resp_median_grand' in f:\n",
    "        del f['resp_median_grand']  # Delete existing dataset if it exists\n",
    "    f.create_dataset('spikerate_grand', data=spikerate_grand_flattened, compression='gzip')\n",
    "    f.create_dataset('resp_median_grand', data=resp_median_grand, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7528b-666d-427c-abeb-5bef6c028003",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(fname_save,'r')\n",
    "a = f['mesopic']['train']['stim_frames']\n",
    "print(a.shape)\n",
    "f.close()\n",
    "\n",
    "# %% STA\n",
    "from model.featureMaps import decompose\n",
    "f = h5py.File(fname_save,'r')\n",
    "stim_frames_train = np.array(f['mesopic']['train']['stim_frames'])\n",
    "spikeRate_train = np.array(f['mesopic']['train']['spikeRate'])\n",
    "spikeRate_train = np.squeeze(np.array(f['mesopic']['train']['spikeCounts']))\n",
    "f.close()\n",
    "\n",
    "# %%\n",
    "idx_data = np.arange(0,10000)\n",
    "data_lim = 100000\n",
    "stim = stim_frames_train[:data_lim]\n",
    "stim = np.reshape(stim,(stim.shape[0],num_checkers_y,num_checkers_x),order='F')       \n",
    "stim = stim-stim.mean()\n",
    "\n",
    "idx_cell = 5\n",
    "spikes = spikeRate_train[:,idx_cell][:data_lim]\n",
    "spikes = np.where(spikes)[0]\n",
    "\n",
    "\n",
    "num_spikes = len(spikes)\n",
    "spikeCount = 0\n",
    "nFrames =100\n",
    "sta = np.zeros((nFrames,stim.shape[1],stim.shape[2]))\n",
    "\n",
    "idx_start = np.where(spikes>nFrames)[0][0]\n",
    "for i in tqdm(range(idx_start,num_spikes)):\n",
    "    if i>spikes.shape[0]:\n",
    "        break\n",
    "    else:\n",
    "        last_frame = spikes[i]\n",
    "        first_frame = last_frame - nFrames\n",
    "        \n",
    "        sta = sta + stim[first_frame:last_frame,:,:]\n",
    "        spikeCount+=1\n",
    "sta = sta/spikeCount\n",
    "\n",
    "spatial_feature, temporal_feature = decompose(sta)\n",
    "# plt.plot(1,2,1)\n",
    "# plt.imshow(spatial_feature,cmap='winter')\n",
    "# plt.plot(1,2,2)\n",
    "# plt.plot(temporal_feature)\n",
    "# plt.title(f'RGC: {uname_all[idx_cell]}, Cell ID {idx_allunits[idx_cell]}')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot spatial feature on the left subplot\n",
    "ax1.imshow(spatial_feature, cmap='gray')\n",
    "ax1.set_title('Spatial Feature')\n",
    "\n",
    "# Plot temporal feature on the right subplot\n",
    "ax2.plot(temporal_feature)\n",
    "ax2.set_title('Temporal Feature')\n",
    "\n",
    "# Add overall title for the figure\n",
    "fig.suptitle(f'RGC: {uname_all[idx_cell]}, Cell ID {idx_allunits[idx_cell]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
