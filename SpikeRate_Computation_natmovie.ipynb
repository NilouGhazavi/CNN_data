{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f0cb78f-9d17-4d32-8b2e-82c79ff508e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import libraries \n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import io\n",
    "import re\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# from global_scripts import spiketools\n",
    "import gc\n",
    "# import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import math\n",
    "import hdf5storage\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/niloughazavi/Documents/GitHub/CNN_data')\n",
    "from WN_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15da3a02-1f5c-44f6-b5e5-800492cc817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment date\n",
    "expDate = '20240229C'\n",
    "\n",
    "# Temporal Resolution: upsampling/downsampling factor determines how finely we want to divide the time axis when processing the stimulus \n",
    "# UP_SAMP=1 --> no changes in temporal resolution and UP_SAMP=0 --> temporal resolution changes \n",
    "UP_SAMP = 0\n",
    "\n",
    "# Spike Times-Gaussian Kernel Convolution \n",
    "# duration of each time bin (smaller time bin = higher temporal resolution for detecting rapid changes) \n",
    "bin_width=8\n",
    "# sig=4 for bin_width=8 ms, sig=2 for bin_width=16 ms, sig=8 for bin_width=4 ms, sig=20 for bin_width=1 ms\n",
    "sig=4 \n",
    "\n",
    "# Light Levels \n",
    "lightLevel_text = ['scotopic','mesopic','photopic']\n",
    "# multiplication factor for each light level \n",
    "lightLevel_mul = [0.5,50,6000]\n",
    "# unit conversion of light level (R* excited state of molecule R) \n",
    "CONVERT_RSTAR = True\n",
    "# stimulus normalization\n",
    "NORM_STIM = 1\n",
    "# RGC response normalization\n",
    "NORM_RESP = True\n",
    "FRAME_RESIZE = 8\n",
    "\n",
    "# Stimulus \n",
    "# refresh rate is 60 Hz, spike bin rate is 120 Hz, the stimulus should be upsampled so each frame is 8 ms ( the same as bin time)\n",
    "stim_upsample = 2 \n",
    "# refresh rate for each frame ( t_frame should be the same as bin time after doubling the temporal resolution (stim_upsample)\n",
    "t_frame=(1/60.3180)*1000/stim_upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fb8919-7ec3-4789-bc96-d6163386f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOVEs MOVIE\n",
    "# total number of movies (unique indices?)\n",
    "total_num_movies= 161\n",
    "\n",
    "# validation dataset\n",
    "idx_movs_val=[0]\n",
    "\n",
    "# Train dataset: Exclude the index of the validation dataset \n",
    "idx_movs_train= np.setdiff1d(np.arange(0,total_num_movies),idx_movs_val)\n",
    "\n",
    "# number of validation movies\n",
    "N_VAL = len(idx_movs_val)   # 2 movies for validation purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcd91eff-a284-48c8-ac82-2d075a5e2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories to movies and raw data\n",
    "\n",
    "path_raw = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'raw')\n",
    "path_movs = os.path.join(path_raw,'movie_files')\n",
    "path_db = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "path_save = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "\n",
    "for path in [path_raw, path_movs, path_db, path_save]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "# create files to save spike rate\n",
    "\n",
    "if CONVERT_RSTAR==True:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_NATSTIM'+str(idx_movs_val[0])+'_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms'+'_Rstar.h5'))\n",
    "else:\n",
    "    fname_save = os.path.join(path_save,(expDate+'_dataset_NATSTIM'+str(idx_movs_val[0])+'_CORR_'+'allLightLevels'+'_'+str(bin_width)+'ms.h5'))\n",
    "\n",
    "\n",
    "if not os.path.exists(path_save):\n",
    "    os.mkdir(path_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f05dba-c48c-44c7-a6ea-965215472a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data for each sheet\n",
    "off_parasol_data = {\n",
    "    'Source_ID': [1, 125, 170, 181, 240, 268, 308, 316, 364, 395, 429, 442, 491, 500, 539, 559, 580, 598, 608, 666, 704, 720, 725, 766, 803, 818, 843, 853, 881, 886, 962, 995, 997, 1050, 1102, 1126, 1151, 1164, 1243, 1245, 1246, 1335, 1368, 1390, 1420, 1456, 1494, 1521, 1523, 1569, 1619, 1656, 1722, 1727, 1770],\n",
    "    'Destination_ID': [1, 97, 161, 145, 193, 208, 234, 1487, 279, 1680, 339, 357, 404, 408, 435, 451, 467, 481, 496, 541, 571, 591, 595, 628, 663, 678, 705, 718, 752, 754, 801, 835, 831, 882, 921, 946, 971, 977, 1048, 1050, 1052, 1120, 1149, 1173, 1196, 1227, 1244, 1238, 1268, 1307, 1347, 1380, 1427, 1432, 183]\n",
    "}\n",
    "\n",
    "off_midget_data = {\n",
    "    'Source_ID': [59, 101, 180, 188, 201, 237, 339, 346, 361, 406, 422, 477, 504, 534, 542, 548, 560, 562, 572, 576, 584, 779, 794, 808, 817, 821, 842, 857, 876, 931, 963, 975, 1016, 1085, 1131, 1174, 1178, 1193, 1215, 1277, 1455, 1489, 1543, 1558, 1598, 1623, 1629, 1636, 1672, 1702, 1717],\n",
    "    'Destination_ID': [43, 68, 146, 153, 162, 192, 257, 263, 277, 316, 334, 389, 409, 429, 437, 443, 453, 454, 462, 466, 468, 638, 652, 668, 677, 681, 702, 1700, 746, 783, 802, 813, 846, 930, 951, 986, 989, 1007, 1042, 1075, 1226, 1240, 1286, 1301, 1302, 1349, 1356, 1367, 1390, 1404, 1423]\n",
    "}\n",
    "\n",
    "on_parasol_data = {\n",
    "    'Source_ID': [19, 183, 433, 456, 464, 537, 547, 552, 615, 621, 656, 665, 684, 765, 785, 795, 845, 892, 926, 930, 1005, 1030, 1080, 1115, 1198, 1212, 1255, 1256, 1317, 1419, 1433, 1452, 1498, 1550, 1603, 1634, 1663, 1703, 1715, 1724, 1940],\n",
    "    'Destination_ID': [103, 149, 342, 367, 377, 433, 282, 447, 502, 509, 531, 536, 556, 624, 644, 654, 706, 757, 780, 781, 841, 862, 907, 933, 1010, 1029, 1058, 1060, 1109, 1195, 1206, 1222, 1250, 1291, 1330, 1362, 1728, 1406, 1420, 1405, 139]\n",
    "}\n",
    "\n",
    "on_midget_data = {\n",
    "    'Source_ID': [178, 418, 783, 826, 841, 847, 852, 896, 946, 1004, 1019, 1042, 1069, 1103, 1104, 1111, 1118, 1124, 1148, 1154, 1183, 1190, 1221, 1236, 1247, 1275, 1279, 1328, 1338, 1427],\n",
    "    'Destination_ID': [144, 328, 640, 682, 701, 709, 717, 760, 792, 840, 849, 874, 901, 922, 923, 931, 938, 944, 970, 972, 998, 1005, 1035, 1043, 1053, 1074, 1082, 1119, 1124, 1201]\n",
    "}\n",
    "\n",
    "\n",
    "## DOVES Movie (Quality controlled cells)\n",
    "ON Midget: array([ 682,  701,  709,  717,  792,  840,  849,  874,  901,  923,  931,\n",
    "        938,  998, 1005, 1035, 1053, 1074])\n",
    "\n",
    "OFF Midget: array([ 146,  153,  192,  277,  316,  389,  429,  443,  453,  454,  468,\n",
    "        638,  681,  702,  783,  802,  846,  989, 1042, 1240, 1349, 1404,\n",
    "       1423])\n",
    "\n",
    "ON Parasol: array([ 509,  556,  624,  654,  706,  781,  862,  907, 1029, 1060, 1109,\n",
    "       1195, 1206, 1222, 1291, 1362])\n",
    "\n",
    "OFF Parasol : array([ 435,  451,  467,  496,  571,  628,  801,  831,  882,  971,  977,\n",
    "       1050, 1149, 1307, 1347, 1427, 1432])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f17d7265-59f6-4609-b1bd-fbf94497c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell IDs\n",
    "photopic_cell_dove = {\n",
    "'OffM': [43, 68, 146, 153, 162, 192, 257, 263, 277, 316, 334, 389, 409, 429, 437, 443, 453, 454, 462, 466, 468, 638, 652, 668, 677, 681, 702, 1700, 746, 783, 802, 813, 846, 930, 951, 986, 989, 1007, 1042, 1075, 1226, 1240, 1286, 1301, 1302, 1349, 1356, 1367, 1390, 1404, 1423],\n",
    "'OffP': [1, 97, 161, 145, 193, 208, 234, 1487, 279, 1680, 339, 357, 404, 408, 435, 451, 467, 481, 496, 541, 571, 591, 595, 628, 663, 678, 705, 718, 752, 754, 801, 835, 831, 882, 921, 946, 971, 977, 1048, 1050, 1052, 1120, 1149, 1173, 1196, 1227, 1244, 1238, 1268, 1307, 1347, 1380, 1427, 1432, 183],\n",
    "'OnM': [144, 328, 640, 682, 701, 709, 717, 760, 792, 840, 849, 874, 901, 922, 923, 931, 938, 944, 970, 972, 998, 1005, 1035, 1043, 1053, 1074, 1082, 1119, 1124, 1201],\n",
    "'OnP': [103, 149, 342, 367, 377, 433, 282, 447, 502, 509, 531, 536, 556, 624, 644, 654, 706, 757, 780, 781, 841, 862, 907, 933, 1010, 1029, 1058, 1060, 1109, 1195, 1206, 1222, 1250, 1291, 1330, 1362, 1728, 1406, 1420, 1405, 139],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "photopic_off_m = photopic_cell_dove['OffM']\n",
    "photopic_off_p = photopic_cell_dove['OffP']\n",
    "photopic_on_m = photopic_cell_dove['OnM']\n",
    "photopic_on_p = photopic_cell_dove['OnP']\n",
    "\n",
    "\n",
    "# only parasol cells \n",
    "uname_all = list()\n",
    "ctr = 0\n",
    "for i in photopic_on_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'on_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "    \n",
    "ctr = 0\n",
    "for i in photopic_off_p:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'off_par_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "ctr = 0\n",
    "for i in photopic_on_m:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'on_m_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "ctr = 0\n",
    "for i in photopic_off_m:\n",
    "    ctr+=1\n",
    "    uname_rgb = 'off_m_%03d'%ctr\n",
    "    uname_all.append(uname_rgb)\n",
    "\n",
    "\n",
    "\n",
    "totalNum_units = len(photopic_on_p) + len(photopic_off_p)+len(photopic_on_m)+len(photopic_off_m)\n",
    "\n",
    "\n",
    "photopic_ids=photopic_on_p+photopic_off_p+photopic_on_m+photopic_off_m\n",
    "photopic_ids=np.array(photopic_ids)\n",
    "\n",
    "idx_allunits=photopic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba37d9e-6ffc-4501-8cb8-39800d2f11c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce54efbd-a1f5-476d-a863-0dc836629d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/161 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'movie_dic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m single_movie_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_movs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoves_img_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(movie_idx) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(single_movie_path)\n\u001b[0;32m---> 46\u001b[0m bg_mean \u001b[38;5;241m=\u001b[39m \u001b[43mmovie_dic\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_mean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# frames = cv2.resize(frames, dsize=(int(frames.shape[1]/FRAME_RESIZE), int(frames.shape[0]/FRAME_RESIZE)), interpolation=cv2.INTER_LINEAR)\u001b[39;00m\n\u001b[1;32m     48\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames[:,::FRAME_RESIZE,::FRAME_RESIZE,\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movie_dic' is not defined"
     ]
    }
   ],
   "source": [
    "# how to update this \n",
    "spikerate_grand = np.empty((0,totalNum_units,9,10))      # [time,cells,stims,trials]   <-- nee to find a way to do this auto\n",
    "\n",
    "\n",
    "\n",
    "lightLevel_idx = 2\n",
    "\n",
    "for lightLevel_idx in [2]:     #[0=0.3; 1=3; 2=30]\n",
    "    select_lightLevel = lightLevel_text[lightLevel_idx]\n",
    "\n",
    "    fname = os.path.join(path_raw,expDate+'_Doves_'+select_lightLevel+'.pkl')\n",
    "    with open(fname,'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    #---- Stim \n",
    "    \n",
    "    spike_dict = params['spike_dict']\n",
    "    cluster_id = params['cluster_id']\n",
    "    stimulus_index = params['stimulusIndex'].astype(int)\n",
    "    background_intensity = params['backgroundIntensity'].astype(float)\n",
    "    frame_times = params['frame_times'] # This is the time of each frame in milliseconds.\n",
    "    pre_pts = params['pre_pts']\n",
    "    stim_pts = params['stim_pts']\n",
    "    tail_pts = params['tail_pts']\n",
    "    wait_time = params['wait_time']\n",
    "    bin_rate = params['bin_rate'] # This is the bin rate for the spike times in Hz.\n",
    "\n",
    "\n",
    "    unique_movs = np.unique(stimulus_index)\n",
    "    N_TRIALS = int(len(stimulus_index)/len(unique_movs))\n",
    "    frames_mat = np.zeros((362,int(600/FRAME_RESIZE),int(800/FRAME_RESIZE),len(unique_movs)));frames_mat[:] = np.nan        # [time,y/8,x/8,img,trial]\n",
    "    spikes_mat = np.zeros((len(unique_movs),N_TRIALS),dtype='object')\n",
    "    ctr_trial = -1*np.ones(N_TRIALS,dtype='int')\n",
    "    \n",
    "    # Loop over movies\n",
    "    m=5\n",
    "    for m in tqdm(range(len(unique_movs))):\n",
    "        movie_idx = unique_movs[m]\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        \n",
    "\n",
    "\n",
    "        single_movie_path = os.path.join(path_movs, 'doves_img_' + str(movie_idx) + '.npy')\n",
    "        frames = np.load(single_movie_path)\n",
    "\n",
    "        # how to add the mean\n",
    "        bg_mean = movie_dic['image_mean']\n",
    "        frames = frames[:,::FRAME_RESIZE,::FRAME_RESIZE,0]\n",
    "        frames = (frames+frames.max())/(frames.max()-frames.min())\n",
    "        \n",
    "        frames_new = (frames - bg_mean) / bg_mean       # convert to  contrast\n",
    "        \n",
    "        frames_mat[:,:,:,movie_loc] = frames_new\n",
    "\n",
    "        frame_rate = 59.94154881781792\n",
    "\n",
    "\n",
    "    if CONVERT_RSTAR == True:\n",
    "        meanIntensity = lightLevel_mul[lightLevel_idx]\n",
    "        frames_rstar = applyLightIntensities(meanIntensity,frames_mat,t_frame)\n",
    "        \n",
    "    stim_frames = np.repeat(frames_rstar,stim_upsample,axis=0).astype('float32')\n",
    "    stim_frames = np.repeat(stim_frames[:,:,:,:,None],N_TRIALS,axis=-1)\n",
    "\n",
    "    # Extract spikes\n",
    "    epoch_count = 0\n",
    "    for epoch_count in tqdm(range(len(stimulus_index))):\n",
    "        movie_idx = int(stimulus_index[epoch_count])\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        ctr_trial[movie_loc] += 1\n",
    "\n",
    "        epoch_frame_times = np.array(frame_times[epoch_count])  # Get the frame times for the epoch.\n",
    "        epoch_frame_times -= epoch_frame_times[0]               # Subtract the first frame.\n",
    "        stim_start = pre_pts[epoch_count]                       # The stimulus starts at the end of pre_pts.\n",
    "        the_wait_is_over = stim_start + wait_time[epoch_count]  # The first movie frame is then presented for the duration of the wait time.\n",
    "        stim_end = stim_start + stim_pts[epoch_count]           # The stimulus then plays for the duration of the stim_pts-wait_time.\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b13848d-1209-4b43-ac8e-c36d4b8c7b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to update this \n",
    "spikerate_grand = np.empty((0,totalNum_units,9,10))      # [time,cells,stims,trials]   <-- nee to find a way to do this auto\n",
    "\n",
    "\n",
    "\n",
    "lightLevel_idx = 2\n",
    "\n",
    "for lightLevel_idx in [2]:     #[0=0.3; 1=3; 2=30]\n",
    "    select_lightLevel = lightLevel_text[lightLevel_idx]\n",
    "\n",
    "    fname = os.path.join(path_raw,expDate+'_Doves_'+select_lightLevel+'.pkl')\n",
    "    with open(fname,'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    \n",
    "    #---- Stim \n",
    "    \n",
    "    spike_dict = params['spike_dict']\n",
    "    cluster_id = params['cluster_id']\n",
    "    stimulus_index = params['stimulusIndex'].astype(int)\n",
    "    background_intensity = params['backgroundIntensity'].astype(float)\n",
    "    frame_times = params['frame_times'] # This is the time of each frame in milliseconds.\n",
    "    pre_pts = params['pre_pts']\n",
    "    stim_pts = params['stim_pts']\n",
    "    tail_pts = params['tail_pts']\n",
    "    wait_time = params['wait_time']\n",
    "    bin_rate = params['bin_rate'] # This is the bin rate for the spike times in Hz.\n",
    "\n",
    "\n",
    "    unique_movs = np.unique(stimulus_index)\n",
    "    N_TRIALS = int(len(stimulus_index)/len(unique_movs))\n",
    "    frames_mat = np.zeros((362,int(600/FRAME_RESIZE),int(800/FRAME_RESIZE),len(unique_movs)));frames_mat[:] = np.nan        # [time,y/8,x/8,img,trial]\n",
    "    spikes_mat = np.zeros((len(unique_movs),N_TRIALS),dtype='object')\n",
    "    ctr_trial = -1*np.ones(N_TRIALS,dtype='int')\n",
    "    \n",
    "    # Loop over movies\n",
    "    m=5\n",
    "    for m in tqdm(range(len(unique_movs))):\n",
    "        movie_idx = unique_movs[m]\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        \n",
    "\n",
    "\n",
    "        single_movie_path = os.path.join(path_movs, 'doves_idx_' + str(movie_idx) + '.npy')\n",
    "        frames = np.load(single_movie_path)\n",
    "    \n",
    "        bg_mean = movie_dic['image_mean']\n",
    "        # frames = cv2.resize(frames, dsize=(int(frames.shape[1]/FRAME_RESIZE), int(frames.shape[0]/FRAME_RESIZE)), interpolation=cv2.INTER_LINEAR)\n",
    "        frames = frames[:,::FRAME_RESIZE,::FRAME_RESIZE,0]\n",
    "        frames = (frames+frames.max())/(frames.max()-frames.min())\n",
    "        \n",
    "        frames_new = (frames - bg_mean) / bg_mean       # convert to  contrast\n",
    "        \n",
    "        frames_mat[:,:,:,movie_loc] = frames_new\n",
    "\n",
    "        frame_rate =  59.94154881781792\n",
    "\n",
    "\n",
    "    if CONVERT_RSTAR == True:\n",
    "        meanIntensity = lightLevel_mul[lightLevel_idx]\n",
    "        frames_rstar = applyLightIntensities(meanIntensity,frames_mat,t_frame)\n",
    "        \n",
    "    stim_frames = np.repeat(frames_rstar,stim_upsample,axis=0).astype('float32')\n",
    "    stim_frames = np.repeat(stim_frames[:,:,:,:,None],N_TRIALS,axis=-1)\n",
    "\n",
    "    # Extract spikes\n",
    "    epoch_count = 0\n",
    "    for epoch_count in tqdm(range(len(stimulus_index))):\n",
    "        movie_idx = int(stimulus_index[epoch_count])\n",
    "        movie_loc = np.where(movie_idx==unique_movs)[0][0]\n",
    "        ctr_trial[movie_loc] += 1\n",
    "\n",
    "        epoch_frame_times = np.array(frame_times[epoch_count])  # Get the frame times for the epoch.\n",
    "        epoch_frame_times -= epoch_frame_times[0]               # Subtract the first frame.\n",
    "        stim_start = pre_pts[epoch_count]                       # The stimulus starts at the end of pre_pts.\n",
    "        the_wait_is_over = stim_start + wait_time[epoch_count]  # The first movie frame is then presented for the duration of the wait time.\n",
    "        stim_end = stim_start + stim_pts[epoch_count]           # The stimulus then plays for the duration of the stim_pts-wait_time.\n",
    "    \n",
    "        # Get the relevant spikes\n",
    "        spikes_epoch = spike_dict[:,epoch_count]\n",
    "        spikes_stim = []\n",
    "        i=0\n",
    "        for i in range(len(spikes_epoch)):\n",
    "             rgb = spikes_epoch[i]\n",
    "             rgb = rgb[rgb>stim_start]\n",
    "             rgb = rgb[rgb<stim_end]\n",
    "             rgb = rgb - stim_start\n",
    "             rgb = rgb[rgb>0]\n",
    "             spikes_stim.append(rgb)\n",
    "        spikes_mat[movie_loc,ctr_trial[movie_loc]] = spikes_stim\n",
    "    \n",
    "    \n",
    "\n",
    "    # ---- spikes\n",
    "    assert len(np.unique(stim_pts))==1, 'stim_pts different for different epochs/movies'\n",
    "\n",
    "    stimLength = stim_frames.shape[0]\n",
    "    \n",
    "    flips = np.arange(0,(stimLength+1)*t_frame,t_frame)\n",
    "    \n",
    "    numStims = len(unique_movs)\n",
    "    spikeRate_cells = np.empty((stimLength,totalNum_units,numStims,N_TRIALS))\n",
    "    spikeCounts_cells = np.empty((stimLength,totalNum_units,numStims,N_TRIALS))\n",
    "    spikeTimes_cells = np.empty((totalNum_units,numStims,N_TRIALS),dtype='object')\n",
    "    \n",
    "    \n",
    "    ctr_units = -1\n",
    "    U = 0\n",
    "    # had to be modified for one light level\n",
    "    for U in range(0, len(idx_allunits)):\n",
    "        ctr_units += 1\n",
    "        cluster_unit = idx_allunits[U]\n",
    "        idx_unit_array = np.where(cluster_id == cluster_unit)[0]\n",
    "        # if len(idx_unit_array) == 0:\n",
    "        #     continue  # Skip if cluster_unit is not found in cluster_id\n",
    "        idx_unit = idx_unit_array[0]  # Convert array to integer\n",
    "        \n",
    "        tr = 0\n",
    "        s = 0\n",
    "        for s in range(numStims):\n",
    "            for tr in range(N_TRIALS):\n",
    "                startTime = 0  # Start time for stimulus\n",
    "                endTime = startTime + flips[-1]  # End time for stimulus\n",
    "                \n",
    "                spikes = spikes_mat[s, tr][idx_unit]\n",
    "                if len(spikes) == 0:\n",
    "                    spikes = 0\n",
    "                    spikeCounts = np.zeros(stimLength)\n",
    "                else:\n",
    "                    spikeCounts = np.histogram(spikes, flips)[0]  # Bin the spikes\n",
    "                spikeRate = MEA_spikerates_binned(spikeCounts, sig)  # Calculate spike rates\n",
    "                \n",
    "                spikeRate_cells[:, ctr_units, s, tr] = spikeRate\n",
    "                spikeCounts_cells[:, ctr_units, s, tr] = spikeCounts\n",
    "                spikeTimes_cells[ctr_units, s, tr] = spikes\n",
    "    \n",
    "\n",
    "\n",
    "    # Normalize responses if required\n",
    "    if NORM_RESP:\n",
    "        rgb = np.squeeze(spikeRate_cells)\n",
    "        rgb[rgb == 0] = np.nan\n",
    "        # resp_median = np.nanmedian(rgb, axis=0)\n",
    "        # nilou added\n",
    "        if np.all(np.isnan(rgb)):\n",
    "            resp_median = np.zeros_like(rgb[0])\n",
    "        else:\n",
    "            resp_median = np.nanmedian(rgb, axis=0)\n",
    "        resp_norm = rgb / resp_median[None, :]\n",
    "        resp_norm[np.isnan(resp_norm)] = 0\n",
    "        resp_orig = spikeRate_cells\n",
    "    else:\n",
    "        resp_norm = spikeRate_cells\n",
    "        resp_orig = spikeRate_cells\n",
    "\n",
    "    # Prepare training and validation datasets\n",
    "    idx_frames_start = 0  # Discard first 121 frames\n",
    "    stim_frames_train = (stim_frames[idx_frames_start:, :, :, idx_movs_train, :], flips[idx_frames_start:])\n",
    "    spikeRate_train = (resp_norm[idx_frames_start:, :, idx_movs_train, :], spikeCounts_cells[idx_frames_start:, :, idx_movs_train, :], spikeTimes_cells[:, idx_movs_train, :])\n",
    "    spikeRate_orig_train = spikeRate_cells[idx_frames_start:, :, idx_movs_train, :]\n",
    "    spikeRate_median_train = resp_median[:, idx_movs_train, :]\n",
    "    unique_movs_train = unique_movs[idx_movs_train]\n",
    "\n",
    "    stim_frames_val = (stim_frames[idx_frames_start:, :, :, idx_movs_val, :], flips[idx_frames_start:])\n",
    "    spikeRate_val = (resp_norm[idx_frames_start:, :, idx_movs_val, :], spikeCounts_cells[idx_frames_start:, :, idx_movs_val, :], spikeTimes_cells[:, idx_movs_val, :])\n",
    "    spikeRate_orig_val = spikeRate_cells[idx_frames_start:, :, idx_movs_val, :]\n",
    "    spikeRate_median_val = resp_median[:, idx_movs_val, :]\n",
    "    unique_movs_val = unique_movs[idx_movs_val]\n",
    "    \n",
    "    # Update grand spikerate\n",
    "    spikerate_grand = np.concatenate((spikerate_grand, spikeRate_cells), axis=0)\n",
    "\n",
    "    # Save dataset\n",
    "    \n",
    "        # Create directories if necessary before saving\n",
    "    os.makedirs(os.path.dirname(fname_save), exist_ok=True)\n",
    "    \n",
    "    with h5py.File(fname_save, 'a') as f:\n",
    "        try:\n",
    "            # Save experimental date and unit information\n",
    "            f.create_dataset('expDate', data=np.array(expDate, dtype='bytes'))\n",
    "            f.create_dataset('units', data=np.array(uname_all, dtype='bytes'))\n",
    "        except:\n",
    "            pass  # If datasets already exist, skip creating them\n",
    "    \n",
    "        # Save training data\n",
    "        grp = f.create_group('/' + select_lightLevel + '/train')\n",
    "        create_dataset(grp, stim_frames_train, spikeRate_train, spikeRate_orig_train, spikeRate_median_train, unique_movs_train, t_frame, N_TRIALS)\n",
    "    \n",
    "        # Save validation data\n",
    "        grp = f.create_group('/' + select_lightLevel + '/val')\n",
    "        create_dataset(grp, stim_frames_val, spikeRate_val, spikeRate_orig_val, spikeRate_median_val, unique_movs_val, t_frame, N_TRIALS)\n",
    "    \n",
    "    # Flatten and reshape spike rate data for saving\n",
    "    spikerate_grand_flattened = np.moveaxis(spikerate_grand, 0, -1)\n",
    "    spikerate_grand_flattened = spikerate_grand_flattened.reshape(spikerate_grand_flattened.shape[0], -1)\n",
    "    resp_median_grand = np.nanmedian(spikerate_grand_flattened, axis=-1)\n",
    "    \n",
    "    # Save grand spike rate and median response data\n",
    "    with h5py.File(fname_save, 'a') as f:\n",
    "        if 'spikerate_grand' in f:\n",
    "            del f['spikerate_grand']  # Delete existing dataset if it exists\n",
    "        if 'resp_median_grand' in f:\n",
    "            del f['resp_median_grand']  # Delete existing dataset if it exists\n",
    "        f.create_dataset('spikerate_grand', data=spikerate_grand_flattened, compression='gzip')\n",
    "        f.create_dataset('resp_median_grand', data=resp_median_grand, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c886d53-ac89-4325-a79d-e8ed6104b46d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
