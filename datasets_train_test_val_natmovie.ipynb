{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57eaeda4-d002-4284-b813-7855b6c80c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save training, testing and validation datasets to be read by jobs on cluster\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "os.chdir('/Users/niloughazavi/Documents/GitHub/RetinaPredictors-main')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy as np\n",
    "from model.data_handler import check_trainVal_contamination\n",
    "from model.data_handler_mike import load_data_allLightLevels_cb, load_data_allLightLevels_natstim, save_h5Dataset\n",
    "from collections import namedtuple\n",
    "Exptdata = namedtuple('Exptdata', ['X', 'y'])\n",
    "Exptdata_spikes = namedtuple('Exptdata', ['X', 'y','spikes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1da219e7-2288-41e1-b09c-77d729f83179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lightLevel = 'allLightLevels'     # ['scotopic', 'photopic','scotopic_photopic']\n",
    "datasetsToLoad = ['mesopic']#,'photopic'];    #['scotopic','photopic','scotopic_photopic']\n",
    "N_split = 0\n",
    "\n",
    "# update the validation index\n",
    "natstim_idx_val =0\n",
    "\n",
    "# Nilou changed it to 1 (it was 6)\n",
    "REP_TRAINING_DATA = 0\n",
    "STIM = 'NATSTIM'+str(natstim_idx_val)+'_CORR' # 'CB'  'NATSTIM'\n",
    "STIM_NAT = 'NATSTIM'+str(natstim_idx_val)+'_CORR'\n",
    "file_suffix = 'Rstar'\n",
    "NORM_STIM = 0\n",
    "NORM_RESP = True\n",
    "D_TYPE = 'f4'\n",
    "\n",
    "\n",
    "expDate = '20240229C'     \n",
    "path_dataset = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "path_save = os.path.join('/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat',expDate,'datasets')\n",
    "    \n",
    "t_frame = 8\n",
    "\n",
    "\n",
    "fname_dataFile = os.path.join(path_dataset,(expDate+'_dataset_'+STIM+'_allLightLevels'+'_'+str(t_frame)+'ms_'+file_suffix+'.h5'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e78bf-e790-4ac6-8a8b-6f0c3f5b1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasetsToLoad[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasetsToLoad:\n",
    "    fname_noise = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM_NAT+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'+'.h5'))\n",
    "\n",
    "\n",
    "    \n",
    "    if STIM[:7] == 'NATSTIM':\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig,_ = load_data_allLightLevels_natstim(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM=False,NORM_RESP=NORM_RESP)\n",
    "\n",
    "\n",
    "        if REP_TRAINING_DATA>0:\n",
    "            print('RESAMPLING TRAINING SAMPLES')\n",
    "            X = np.tile(data_train.X,[REP_TRAINING_DATA,1,1,1,1])\n",
    "            y = np.tile(data_train.y,[REP_TRAINING_DATA,1,1,1])\n",
    "            spikes = np.tile(data_train.spikes,[REP_TRAINING_DATA,1,1,1])\n",
    "            \n",
    "            data_train = Exptdata_spikes(X,y,spikes)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        # if data_quality['var_noise']==None:\n",
    "        #     with h5py.File(fname_noise) as f:\n",
    "        #         obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        #     data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "            \n",
    "    elif 'CB' in STIM:\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig = load_data_allLightLevels_cb(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM = False,NORM_RESP=NORM_RESP)\n",
    "        \n",
    "        \n",
    "        with h5py.File(fname_noise) as f:\n",
    "            obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "    if REP_TRAINING_DATA>0:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_REP-'+str(REP_TRAINING_DATA)+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    else:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    \n",
    "    f = h5py.File(fname_dataFile,'r')\n",
    "    samps_shift = 0#np.array(f[dataset]['val']['spikeRate'].attrs['samps_shift'])\n",
    "    if 'num_checkers_x' in f[dataset]['train']['stim_frames'].attrs.keys():\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_x'])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_y'])\n",
    "        checkSize_um = np.array(f[dataset]['train']['stim_frames'].attrs['checkSize_um'])\n",
    "    else:\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].shape[2])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].shape[1])\n",
    "        checkSize_um = 3.8  # 3.8 um/pixel\n",
    "\n",
    "\n",
    "    t_frame_inData = np.array(f[dataset]['train']['stim_frames'].attrs['t_frame'])\n",
    "    parameters = {\n",
    "    't_frame': t_frame_inData,\n",
    "    'filt_temporal_width': filt_temporal_width,\n",
    "    'frac_val': frac_val,\n",
    "    'frac_test':frac_test,\n",
    "    'thresh_rr': thresh_rr,\n",
    "    'samps_shift': samps_shift,\n",
    "    'num_checkers_x': num_checkers_x,\n",
    "    'num_checkers_y': num_checkers_y,\n",
    "    'checkSize_um': checkSize_um\n",
    "    }\n",
    "    f.close()\n",
    "    \n",
    "    if data_train.X.ndim == 2:\n",
    "       data_train = stim_vecToMat(data_train,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_val = stim_vecToMat(data_val,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_test = stim_vecToMat(data_test,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "    \n",
    "    # fname_data_train_val_test = fname_data_train_val_test + '_StimNorm-'+str(NORM_STIM) #+ '_RespNorm-'+str(NORM_RESP)\n",
    "\n",
    "\n",
    "    save_h5Dataset(fname_data_train_val_test+'.h5',data_train,data_val,data_test,data_quality,dataset_rr,parameters,resp_orig=resp_orig,dtype=D_TYPE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430cba4f-16f3-4990-8d78-8e7a0393e864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for file: /Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat/20240229C/datasets/20240229C_dataset_NATSTIM0_CORR_allLightLevels_8ms_Rstar.h5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat/20240229C/datasets/20240229C_dataset_NATSTIM0_CORR_allLightLevels_8ms_Rstar.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_dataset, (expDate \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dataset_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m STIM \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_allLightLevels\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(t_frame) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mms_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file_suffix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     34\u001b[0m     spikerate_grand \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((spikerate_grand,np\u001b[38;5;241m.\u001b[39marray(f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspikerate_grand\u001b[39m\u001b[38;5;124m'\u001b[39m])),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m spikerate_grand[spikerate_grand\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn_env/lib/python3.9/site-packages/h5py/_hl/files.py:564\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    556\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    557\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    558\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    559\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    560\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    561\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    562\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    563\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 564\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/cnn_env/lib/python3.9/site-packages/h5py/_hl/files.py:238\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    237\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 238\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    240\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/Users/niloughazavi/Documents/Mike_Data/Gradients/RGC_Selective_Stimulation/Natural_Movies/analyses_parasol_midget_cells/data_mike_nat/20240229C/datasets/20240229C_dataset_NATSTIM0_CORR_allLightLevels_8ms_Rstar.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "filt_temporal_width = 0\n",
    "idx_cells = None\n",
    "thresh_rr = 0\n",
    "\n",
    "\n",
    "frac_val = 0.05\n",
    "frac_test = 0.01 \n",
    "\n",
    "\n",
    "    \n",
    "def stim_vecToMat(data,num_checkers_y,num_checkers_x):\n",
    "    X = data.X\n",
    "    X = np.reshape(X,(X.shape[0],num_checkers_y,num_checkers_x),order='F')    # convert stim frames back into spatial dimensions\n",
    "    data = Exptdata_spikes(X,data.y,data.spikes)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# % Calculate grand response median\n",
    "totalNum_units =84\n",
    "spikerate_grand = np.empty((totalNum_units,0))\n",
    "\n",
    "STIMS_ALL = ('NATSTIM'+str(natstim_idx_val)+'_CORR' ,'CB_CORR')\n",
    "\n",
    "\n",
    "#STIM = STIMS_ALL # Set STIM to the current element in STIMS_ALL\n",
    "fname = os.path.join(path_dataset, (expDate + '_dataset_' + STIM + '_allLightLevels' + '_' + str(t_frame) + 'ms_' + file_suffix + '.h5'))\n",
    "print(f\"Looking for file: {fname}\")\n",
    "\n",
    "\n",
    "\n",
    "with h5py.File(fname,'r') as f:\n",
    "    spikerate_grand = np.concatenate((spikerate_grand,np.array(f['spikerate_grand'])),axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "spikerate_grand[spikerate_grand==0]=np.nan\n",
    "resp_med_grand = np.nanmedian(spikerate_grand,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c5078-cf2c-4dfd-a632-3daa2832693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasetsToLoad[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for dataset in datasetsToLoad:\n",
    "    fname_noise = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM_NAT+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'+'.h5'))\n",
    "\n",
    "\n",
    "    \n",
    "    if STIM[:7] == 'NATSTIM':\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig,_ = load_data_allLightLevels_natstim(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM=False,NORM_RESP=NORM_RESP)\n",
    "\n",
    "\n",
    "        if REP_TRAINING_DATA>0:\n",
    "            print('RESAMPLING TRAINING SAMPLES')\n",
    "            X = np.tile(data_train.X,[REP_TRAINING_DATA,1,1,1,1])\n",
    "            y = np.tile(data_train.y,[REP_TRAINING_DATA,1,1,1])\n",
    "            spikes = np.tile(data_train.spikes,[REP_TRAINING_DATA,1,1,1])\n",
    "            \n",
    "            data_train = Exptdata_spikes(X,y,spikes)\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "        # if data_quality['var_noise']==None:\n",
    "        #     with h5py.File(fname_noise) as f:\n",
    "        #         obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        #     data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "            \n",
    "    elif 'CB' in STIM:\n",
    "        data_train,data_val,data_test,data_quality,dataset_rr,resp_orig = load_data_allLightLevels_cb(fname_dataFile,dataset,frac_val=frac_val,frac_test=frac_test,\n",
    "                                                                                                   filt_temporal_width=filt_temporal_width,idx_cells_orig=idx_cells,\n",
    "                                                                                                   resp_med_grand=resp_med_grand,thresh_rr=thresh_rr,N_split=N_split,\n",
    "                                                                                                   CHECK_CONTAM = False,NORM_RESP=NORM_RESP)\n",
    "        \n",
    "        \n",
    "        with h5py.File(fname_noise) as f:\n",
    "            obs_noise = np.array(f['data_quality']['var_noise'])\n",
    "\n",
    "\n",
    "        data_quality['var_noise'] =  obs_noise\n",
    "\n",
    "\n",
    "    if REP_TRAINING_DATA>0:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_REP-'+str(REP_TRAINING_DATA)+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    else:\n",
    "        fname_data_train_val_test = os.path.join(path_save,(expDate+'_dataset_train_val_test_'+STIM+'_'+dataset+'-'+file_suffix+'_'+D_TYPE+'_'+str(t_frame)+'ms'))\n",
    "    \n",
    "    f = h5py.File(fname_dataFile,'r')\n",
    "    samps_shift = 0#np.array(f[dataset]['val']['spikeRate'].attrs['samps_shift'])\n",
    "    if 'num_checkers_x' in f[dataset]['train']['stim_frames'].attrs.keys():\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_x'])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].attrs['num_checkers_y'])\n",
    "        checkSize_um = np.array(f[dataset]['train']['stim_frames'].attrs['checkSize_um'])\n",
    "    else:\n",
    "        num_checkers_x = np.array(f[dataset]['train']['stim_frames'].shape[2])\n",
    "        num_checkers_y = np.array(f[dataset]['train']['stim_frames'].shape[1])\n",
    "        checkSize_um = 3.8  # 3.8 um/pixel\n",
    "\n",
    "\n",
    "    t_frame_inData = np.array(f[dataset]['train']['stim_frames'].attrs['t_frame'])\n",
    "    parameters = {\n",
    "    't_frame': t_frame_inData,\n",
    "    'filt_temporal_width': filt_temporal_width,\n",
    "    'frac_val': frac_val,\n",
    "    'frac_test':frac_test,\n",
    "    'thresh_rr': thresh_rr,\n",
    "    'samps_shift': samps_shift,\n",
    "    'num_checkers_x': num_checkers_x,\n",
    "    'num_checkers_y': num_checkers_y,\n",
    "    'checkSize_um': checkSize_um\n",
    "    }\n",
    "    f.close()\n",
    "    \n",
    "    if data_train.X.ndim == 2:\n",
    "       data_train = stim_vecToMat(data_train,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_val = stim_vecToMat(data_val,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "       data_test = stim_vecToMat(data_test,parameters['num_checkers_y'],parameters['num_checkers_x'])\n",
    "    \n",
    "    # fname_data_train_val_test = fname_data_train_val_test + '_StimNorm-'+str(NORM_STIM) #+ '_RespNorm-'+str(NORM_RESP)\n",
    "\n",
    "\n",
    "    save_h5Dataset(fname_data_train_val_test+'.h5',data_train,data_val,data_test,data_quality,dataset_rr,parameters,resp_orig=resp_orig,dtype=D_TYPE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
